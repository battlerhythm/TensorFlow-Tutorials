{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class DQN:\n",
    "    REPLAY_MEMORY = 10000\n",
    "    BATCH_SIZE = 32\n",
    "    GAMMA = 0.99\n",
    "    STATE_LEN = 4\n",
    "    \n",
    "    def __init__(self, session, width, height, n_action):\n",
    "        self.session = session\n",
    "        self.n_action = n_action\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.memory = deque()\n",
    "        self.state = None\n",
    "        self.input_X = tf.placeholder(tf.float32, [None, width, height, self.STATE_LEN])\n",
    "        self.input_A = tf.placeholder(tf.int64, [None])\n",
    "        self.input_Y = tf.placeholder(tf.float32, [None])\n",
    "        self.Q = self._build_network('main')\n",
    "        self.cost, self.train_op = self._build_op()\n",
    "        self.target_Q = self._build_network('target')\n",
    "        \n",
    "    def _build_network(self, name):\n",
    "        with tf.variable_scope(name):\n",
    "            model = tf.layers.conv2d(self.input_X, 32, [4, 4], padding='same', activation=tf.nn.relu)\n",
    "            model = tf.layers.conv2d(model, 64, [2, 2], padding='same', activation=tf.nn.relu)\n",
    "            model = tf.contrib.layers.flatten(model)\n",
    "            model = tf.layers.dense(model, 512, activation=tf.nn.relu)\n",
    "            \n",
    "            Q = tf.layers.dense(model, self.n_action, activation=None)\n",
    "            \n",
    "        return Q\n",
    "    \n",
    "    def _build_op(self):\n",
    "        one_hot = tf.one_hot(self.input_A, self.n_action, 1.0, 0.0)\n",
    "        Q_value = tf.reduce_sum(tf.multiply(self.Q, one_hot), axis=1)\n",
    "        cost = tf.reduce_mean(tf.square(self.input_Y - Q_value))\n",
    "        train_op = tf.train.AdamOptimizer(1e-6).minimize(cost)\n",
    "        \n",
    "        return cost, train_op\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        copy_op = []\n",
    "        \n",
    "        main_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='main')\n",
    "        target_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='target')\n",
    "        \n",
    "        for main_var, target_var in zip(main_vars, target_vars):\n",
    "            copy_op.append(target_var.assign(main_var.value()))\n",
    "            \n",
    "        self.session.run(copy_op)\n",
    "        \n",
    "    def get_action(self):\n",
    "        Q_value = self.session.run(self.Q, feed_dict={self.input_X: [self.state]})\n",
    "        \n",
    "        action = np.argmax(Q_value[0])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def init_state(self, state):\n",
    "        state = [state for _ in range(self.STATE_LEN)]\n",
    "        self.state = np.stack(state, axis=2)\n",
    "        \n",
    "    def remember(self, state, action, reward, terminal):\n",
    "        next_state = np.reshape(state, (self.width, self.height, 1))\n",
    "        next_state = np.append(self.state[:, :, 1:], next_state, axis=2)\n",
    "        \n",
    "        self.memory.append((self.state, next_state, action, reward, terminal))\n",
    "\n",
    "        if len(self.memory) > self.REPLAY_MEMORY:\n",
    "            self.memory.popleft()\n",
    "            \n",
    "        self.state = next_state\n",
    "        \n",
    "    def _sample_memory(self):\n",
    "        sample_memory = random.sample(self.memory, self.BATCH_SIZE)\n",
    "        \n",
    "        state = [memory[0] for memory in sample_memory]\n",
    "        next_state = [memory[1] for memory in sample_memory]\n",
    "        action = [memory[2] for memory in sample_memory]\n",
    "        reward = [memory[3] for memory in sample_memory]\n",
    "        terminal = [memory[4] for memory in sample_memory]\n",
    "        \n",
    "        return state, next_state, action, reward, terminal\n",
    "    \n",
    "    def train(self):\n",
    "        state, next_state, action, reward, terminal = self._sample_memory()\n",
    "        target_Q_value = self.session.run(self.target_Q, feed_dict={self.input_X: next_state})\n",
    "        \n",
    "        Y = []\n",
    "        for i in range(self.BATCH_SIZE):\n",
    "            if terminal[i]:\n",
    "                Y.append(reward[i])\n",
    "            else:\n",
    "                Y.append(reward[i] + self.GAMMA * np.max(target_Q_value[i]))\n",
    "\n",
    "                \n",
    "        self.session.run(self.train_op, feed_dict={self.input_X: state, self.input_A: action, self.input_Y: Y})\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
